{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "pRF-gtU1kac2",
    "outputId": "1349656e-3441-4f45-d066-5cc5e9272d8c"
   },
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import logging\n",
    "import os\n",
    "from pprint import pprint\n",
    "import shutil\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from ssd.common.callbacks import CallbackBuilder\n",
    "from ssd.common.distribute import get_strategy\n",
    "from ssd.common.config import load_config\n",
    "from ssd.common.viz_utils import draw_boxes_cv2, imshow, visualize_detections\n",
    "from ssd.data.dataset_builder import DatasetBuilder\n",
    "from ssd.losses.multibox_loss import MultiBoxLoss\n",
    "from ssd.models.ssd_model import SSDModel\n",
    "\n",
    "logger = tf.get_logger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "logger.info('version: {}'.format(tf.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cPB_Vmxwkac7"
   },
   "outputs": [],
   "source": [
    "# !python3 ssd/scripts/calculate_feature_shapes.py --image_height 512 --image_width 512 --num_feature_maps 7\n",
    "# !python3 ssd/scripts/calculate_scales.py -n 7 --s_first 0.04 --smin 0.1 --smax 0.9\n",
    "# !python3 check_matching.py ssd/cfg/coco_resnet_50_512x512-16.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "yIfD1wimkac_",
    "outputId": "c65f657d-67eb-4ccb-cd78-6ef3e8b21426"
   },
   "outputs": [],
   "source": [
    "config = load_config('ssd/cfg/coco_resnet_50_512x512-8.yaml')\n",
    "# config['use_mixed_precision'] = False\n",
    "# config['use_tpu'] = False\n",
    "config['augment_val_dataset'] = False\n",
    "config['resume_training'] = False\n",
    "\n",
    "if config['use_mixed_precision']:\n",
    "    if config['use_tpu']:\n",
    "        dtype = 'mixed_bfloat16'\n",
    "    elif config['use_gpu']:\n",
    "#         dtype = 'mixed_float16' # todo: implement loss scaling\n",
    "        dtype = 'float32'\n",
    "else:\n",
    "    dtype = 'float32'\n",
    "        \n",
    "policy = tf.keras.mixed_precision.experimental.Policy(dtype)\n",
    "tf.keras.mixed_precision.experimental.set_policy(policy)\n",
    "\n",
    "logger.info('\\nCompute dtype: {}'.format(policy.compute_dtype))\n",
    "logger.info('Variable dtype: {}'.format(policy.variable_dtype))\n",
    "\n",
    "strategy = get_strategy(config)\n",
    "\n",
    "epochs = config['epochs']\n",
    "\n",
    "lr_values = list(config['lr_values'])\n",
    "if config['scale_lr']:\n",
    "    for i in range(len(lr_values)):\n",
    "        lr_values[i] *= strategy.num_replicas_in_sync\n",
    "config['lr_values'] = lr_values\n",
    "\n",
    "batch_size = config['batch_size']\n",
    "batch_size = batch_size if not config['scale_batch_size'] else batch_size * strategy.num_replicas_in_sync\n",
    "config['batch_size'] = batch_size\n",
    "\n",
    "train_steps = config['train_images'] // config['batch_size']\n",
    "val_steps = config['val_images'] // config['batch_size']\n",
    "\n",
    "print('\\n')\n",
    "pprint(config, width=120, compact=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config['clear_previous_runs']:\n",
    "    if config['use_tpu']:\n",
    "        logger.warning('Skipping GCS Bucket')\n",
    "    else:\n",
    "        try:\n",
    "            shutil.rmtree(os.path.join(config['model_dir']))\n",
    "            logger.info('Cleared existing model files\\n')\n",
    "        except FileNotFoundError:\n",
    "            logger.warning('model_dir not found!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "xxDv9velkadC",
    "outputId": "61605fa6-09fb-4bf8-aa12-e30cc2cc5bc0"
   },
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    train_dataset = DatasetBuilder('train', config)\n",
    "    val_dataset = DatasetBuilder('val', config)\n",
    "\n",
    "    loss_fn = MultiBoxLoss(config)\n",
    "    lr_sched = tf.optimizers.schedules.PiecewiseConstantDecay(config['lr_boundaries'], config['lr_values'])\n",
    "    optimizer = tf.optimizers.SGD(lr_sched, momentum=config['optimizer_momentum'], nesterov=config['nestrov'])\n",
    "    callbacks_list = CallbackBuilder('_COCO_', config).get_callbacks()\n",
    "\n",
    "    model = SSDModel(config)\n",
    "    model.compile(loss_fn=loss_fn, optimizer=optimizer)\n",
    "    if config['resume_training']:\n",
    "        latest_checkpoint = tf.train.latest_checkpoint(os.path.join(config['model_dir'], 'checkpoints'))\n",
    "        if latest_checkpoint is not None:\n",
    "            logger.info('Loading weights from {}'.format(latest_checkpoint))\n",
    "            model.load_weights(latest_checkpoint)\n",
    "        else:\n",
    "            logger.warning('No weights found, training from scratch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.fit(train_dataset.dataset,\n",
    "#           epochs=epochs,\n",
    "#           steps_per_epoch=train_steps,\n",
    "#           validation_data=val_dataset.dataset,\n",
    "#           validation_steps=val_steps,\n",
    "#           callbacks=callbacks_list)\n",
    "\n",
    "# with strategy.scope():\n",
    "#     save_path = os.path.join(config['model_dir'], 'final_weights', 'ssd_weights')\n",
    "#     logger.info('Saving final weights at in {}'.format(save_path))\n",
    "#     model.save_weights(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    latest_checkpoint = tf.train.latest_checkpoint(os.path.join(config['model_dir'] , 'best_weights'))\n",
    "    logger.info('Loading weights from {}'.format(latest_checkpoint))\n",
    "    model.load_weights(latest_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for images, _ in val_dataset.dataset.take(5):\n",
    "    for i in tqdm(range(images.shape[0])):\n",
    "        image = tf.cast(images[i], dtype=policy.compute_dtype)\n",
    "        detections = model.predict(image[None, ...])\n",
    "        image = (image + tf.constant([103.939, 116.779, 123.68]))[:, :, ::-1]\n",
    "        categories = [config['classes'][cls_id] for cls_id in detections['cls_ids']]\n",
    "        ax = visualize_detections(image, detections['boxes'], categories, detections['scores'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = 'assets/000_G96XV-640x400.jpg'\n",
    "\n",
    "image = tf.io.read_file(image_path)\n",
    "image = tf.image.decode_image(image)\n",
    "image = tf.cast(image, tf.float32)\n",
    "\n",
    "# image = random_pad_to_square(image)\n",
    "image = tf.image.resize(image, [config['image_height'], config['image_width']])\n",
    "\n",
    "image_preprocessed = image[:, :, ::-1] - tf.constant([103.939, 116.779, 123.68])\n",
    "\n",
    "s = time()\n",
    "detections = model.predict(image_preprocessed[None, ...])\n",
    "e = time()\n",
    "logger.info('Inference time: {:.3f}'.format(e - s))\n",
    "categories = [config['classes'][cls_id] for cls_id in detections['cls_ids']]\n",
    "image = draw_boxes_cv2(image, detections['boxes'], categories, thickness=1)\n",
    "imshow(image / 255, (16, 16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "include_colab_link": true,
   "name": "colab_train.ipynb",
   "provenance": []
  },
  "environment": {
   "name": "tf2-cpu.2-1.m47",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-cpu.2-1:m47"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
